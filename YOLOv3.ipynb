{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788d6ab0-900c-4647-a2ed-27966a160b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96f1516-23fc-4b4e-9768-c5ed7277fcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd85ce8-8243-4fd3-bbff-d3c74d4d1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f86647-dad0-4307-b77b-78ebdc50c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "005883d5-417a-49b2-bf2d-b5d9d0bbaaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"TORCH_CPP_LOG_LEVEL\"] = \"INFO\"\n",
    "# os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a12265-f2c5-49f6-a347-f96114cc6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\" to  \"7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d6a587a-cec5-4310-96f2-d1cfa9e2a0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch=2.0.1+cu117, Lightening=2.0.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "print(f\"Torch={torch.__version__}, Lightening={pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d37e1e2-e871-4110-a62a-62ed4525b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parrotletml import config\n",
    "\n",
    "# from parrotletml.utils import seed_everything\n",
    "# seed_everything()  # If you want deterministic behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b7c81-5c37-4314-8199-445115ea684b",
   "metadata": {},
   "source": [
    "### YOLODataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b721ab-dedc-45f2-b92d-b61a34f68dda",
   "metadata": {},
   "source": [
    "#### DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d2193e-f413-4929-b9bd-114d1c88ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parrotletml.datamodule import YOLODataModule\n",
    "\n",
    "data_module = YOLODataModule(\n",
    "    DATASET=config.DATASET,\n",
    "    ANCHORS=config.ANCHORS,\n",
    "    class_names=config.PASCAL_CLASSES,\n",
    "    IMAGE_SIZE=config.IMAGE_SIZE,\n",
    "    TRAIN_IMAGE_SIZES=config.TARGET_IMAGE_SIZE,\n",
    "    S=config.SA,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY,\n",
    "    train_transforms=config.train_transforms,\n",
    "    test_transforms=config.test_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd23cc89-0df8-47db-975a-a775a05249b7",
   "metadata": {},
   "source": [
    "#### Print Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "206d4080-0168-44e7-9c65-52c13b526f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# from parrotletml.utils import (\n",
    "#     plot_couple_examples,\n",
    "#     plot_image,\n",
    "# )\n",
    "\n",
    "# # S = config.S\n",
    "# # anchors = config.ANCHORS\n",
    "# # scaled_anchors = torch.tensor(anchors) / (\n",
    "# #     1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "# # )\n",
    "\n",
    "# number_of_samples = 5\n",
    "# counter = 0\n",
    "\n",
    "# data_module.setup(\"fit\")\n",
    "\n",
    "# # from parrotletml.model import YOLOv3\n",
    "\n",
    "# # num_classes = 20\n",
    "# # ym = YOLOv3(num_classes=num_classes)\n",
    "\n",
    "# for x, y, S in data_module.train_dataloader()[0]:\n",
    "#     counter = counter + 1\n",
    "#     boxes = []\n",
    "\n",
    "#     print(x.shape)\n",
    "#     print(y[0].shape)\n",
    "\n",
    "#     # out = ym(x[0].unsqueeze(0))\n",
    "\n",
    "#     # print(out[0].shape)\n",
    "\n",
    "#     break\n",
    "\n",
    "#     anchors = config.ANCHORS\n",
    "#     scaled_anchors = torch.tensor(anchors).unsqueeze(0) / (\n",
    "#         1\n",
    "#         / torch.stack(S)\n",
    "#         .unsqueeze(1)\n",
    "#         .unsqueeze(1)\n",
    "#         .repeat(1, 3, 2, 1)\n",
    "#         .permute(3, 0, 1, 2)\n",
    "#     )\n",
    "\n",
    "#     for i in range(y[0].shape[1]):\n",
    "#         anchor = scaled_anchors[0][i]\n",
    "#         boxes += cells_to_bboxes(y[i], is_preds=False, S=y[i].shape[2], anchors=anchor)[\n",
    "#             0\n",
    "#         ]\n",
    "#     boxes = non_max_suppression(\n",
    "#         boxes, iou_threshold=1, threshold=0.7, box_format=\"midpoint\"\n",
    "#     )\n",
    "\n",
    "#     plot_image(x[0].permute(1, 2, 0).detach().to(\"cpu\"), boxes)\n",
    "\n",
    "#     if counter >= number_of_samples:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc97ee-5b9e-406e-9dc9-2c350ca00f48",
   "metadata": {},
   "source": [
    "### PL YoloModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f888c58-c766-4fdc-ab12-184c888f732f",
   "metadata": {},
   "source": [
    "#### YoloLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b2e3167-4465-4419-bd47-980d5b647d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parrotletml.loss import YoloLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f852d0-c58d-4c77-9b4c-c30d5d15de95",
   "metadata": {},
   "source": [
    "#### Yolo MAP Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8455445-edf9-4309-aff7-f8946bcc0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parrotletml.calculatemap import CalculateMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb972247-e977-4025-9bfb-8f4a028ba4ea",
   "metadata": {},
   "source": [
    "#### Yolo PL Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "330ac77c-91e7-48ca-89a3-085dcd0e68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parrotletml.yoloplnet import LitYOLONet\n",
    "\n",
    "model = LitYOLONet(\n",
    "    device=config.DEVICE,\n",
    "    lr=config.LEARNING_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747eed5f-8893-4607-b24b-2c02a8b6ac82",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b15b6e4-c432-402b-9dcf-a67f09480160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:lightning_fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO:pytorch_lightning.utilities.rank_zero:----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Loading `train_dataloader` to estimate number of stepping batches.\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name     | Type     | Params\n",
      "--------------------------------------\n",
      "0 | yolo     | YOLOv3   | 61.6 M\n",
      "1 | criteria | YoloLoss | 0     \n",
      "--------------------------------------\n",
      "61.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "61.6 M    Total params\n",
      "246.504   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 518/518 [06:03<00:00,  1.43it/s, v_num=314]      \n",
      "Epoch 1: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=13.50, val_class_accuracy=38.20, val_no_obj_accuracy=26.10, val_obj_accuracy=97.10, train_loss=32.40, train_class_accuracy=27.90, train_no_obj_accuracy=5.490, train_obj_accuracy=98.60]\n",
      "Epoch 2: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=10.30, val_class_accuracy=47.30, val_no_obj_accuracy=79.90, val_obj_accuracy=92.00, train_loss=23.70, train_class_accuracy=36.10, train_no_obj_accuracy=61.30, train_obj_accuracy=93.60]\n",
      "Epoch 3: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=9.420, val_class_accuracy=49.10, val_no_obj_accuracy=85.00, val_obj_accuracy=91.20, train_loss=20.50, train_class_accuracy=41.20, train_no_obj_accuracy=78.60, train_obj_accuracy=93.20]\n",
      "Epoch 4: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=8.580, val_class_accuracy=54.00, val_no_obj_accuracy=85.50, val_obj_accuracy=90.00, train_loss=18.30, train_class_accuracy=46.40, train_no_obj_accuracy=81.50, train_obj_accuracy=93.60]\n",
      "Epoch 5: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=7.790, val_class_accuracy=57.50, val_no_obj_accuracy=84.80, val_obj_accuracy=95.00, train_loss=16.90, train_class_accuracy=50.30, train_no_obj_accuracy=83.30, train_obj_accuracy=94.00]\n",
      "Epoch 6: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=7.240, val_class_accuracy=61.00, val_no_obj_accuracy=86.60, val_obj_accuracy=94.90, train_loss=15.60, train_class_accuracy=54.20, train_no_obj_accuracy=84.70, train_obj_accuracy=94.00]\n",
      "Epoch 7: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=6.170, val_class_accuracy=68.90, val_no_obj_accuracy=88.50, val_obj_accuracy=94.50, train_loss=14.50, train_class_accuracy=57.70, train_no_obj_accuracy=86.00, train_obj_accuracy=94.30]\n",
      "Epoch 8: 100%|██████████| 518/518 [06:01<00:00,  1.43it/s, v_num=314, val_loss=7.650, val_class_accuracy=59.70, val_no_obj_accuracy=89.10, val_obj_accuracy=91.40, train_loss=13.70, train_class_accuracy=60.20, train_no_obj_accuracy=86.80, train_obj_accuracy=94.40]\n",
      "Epoch 9: 100%|██████████| 518/518 [06:01<00:00,  1.43it/s, v_num=314, val_loss=5.920, val_class_accuracy=70.40, val_no_obj_accuracy=91.20, val_obj_accuracy=93.90, train_loss=13.00, train_class_accuracy=62.50, train_no_obj_accuracy=87.40, train_obj_accuracy=94.70]\n",
      "Epoch 10: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=6.260, val_class_accuracy=69.90, val_no_obj_accuracy=90.40, val_obj_accuracy=92.90, train_loss=12.50, train_class_accuracy=64.50, train_no_obj_accuracy=87.80, train_obj_accuracy=94.90]\n",
      "Epoch 11: 100%|██████████| 518/518 [06:01<00:00,  1.43it/s, v_num=314, val_loss=5.730, val_class_accuracy=70.70, val_no_obj_accuracy=90.30, val_obj_accuracy=94.20, train_loss=12.20, train_class_accuracy=65.60, train_no_obj_accuracy=88.20, train_obj_accuracy=95.00]\n",
      "Epoch 12: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=4.960, val_class_accuracy=76.70, val_no_obj_accuracy=91.70, val_obj_accuracy=93.50, train_loss=11.70, train_class_accuracy=67.30, train_no_obj_accuracy=88.60, train_obj_accuracy=95.10]\n",
      "Epoch 13: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=4.870, val_class_accuracy=78.50, val_no_obj_accuracy=90.80, val_obj_accuracy=95.90, train_loss=11.30, train_class_accuracy=68.90, train_no_obj_accuracy=88.90, train_obj_accuracy=95.20]\n",
      "Epoch 14: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=4.620, val_class_accuracy=80.20, val_no_obj_accuracy=91.00, val_obj_accuracy=95.60, train_loss=11.00, train_class_accuracy=69.80, train_no_obj_accuracy=89.20, train_obj_accuracy=95.30]\n",
      "Epoch 15: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=4.680, val_class_accuracy=78.70, val_no_obj_accuracy=93.30, val_obj_accuracy=93.40, train_loss=10.60, train_class_accuracy=71.30, train_no_obj_accuracy=89.50, train_obj_accuracy=95.40]\n",
      "Epoch 16: 100%|██████████| 518/518 [06:01<00:00,  1.43it/s, v_num=314, val_loss=5.240, val_class_accuracy=74.40, val_no_obj_accuracy=92.30, val_obj_accuracy=94.10, train_loss=10.40, train_class_accuracy=72.20, train_no_obj_accuracy=89.80, train_obj_accuracy=95.40]\n",
      "Epoch 17: 100%|██████████| 518/518 [06:03<00:00,  1.43it/s, v_num=314, val_loss=4.390, val_class_accuracy=80.50, val_no_obj_accuracy=93.00, val_obj_accuracy=95.40, train_loss=10.00, train_class_accuracy=73.40, train_no_obj_accuracy=90.10, train_obj_accuracy=95.60]\n",
      "Epoch 18: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=4.370, val_class_accuracy=80.60, val_no_obj_accuracy=92.10, val_obj_accuracy=96.10, train_loss=9.700, train_class_accuracy=74.50, train_no_obj_accuracy=90.30, train_obj_accuracy=95.50]\n",
      "Epoch 19: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=4.230, val_class_accuracy=81.90, val_no_obj_accuracy=92.00, val_obj_accuracy=95.80, train_loss=9.460, train_class_accuracy=75.30, train_no_obj_accuracy=90.50, train_obj_accuracy=95.60]\n",
      "Epoch 20: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=4.070, val_class_accuracy=83.70, val_no_obj_accuracy=91.80, val_obj_accuracy=96.50, train_loss=9.220, train_class_accuracy=76.30, train_no_obj_accuracy=90.70, train_obj_accuracy=95.80]\n",
      "Epoch 21: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=4.100, val_class_accuracy=82.60, val_no_obj_accuracy=93.50, val_obj_accuracy=95.20, train_loss=8.950, train_class_accuracy=77.30, train_no_obj_accuracy=90.90, train_obj_accuracy=95.80]\n",
      "Epoch 22: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=3.800, val_class_accuracy=84.60, val_no_obj_accuracy=93.10, val_obj_accuracy=95.20, train_loss=8.610, train_class_accuracy=78.40, train_no_obj_accuracy=91.20, train_obj_accuracy=95.90]\n",
      "Epoch 23: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=3.660, val_class_accuracy=85.90, val_no_obj_accuracy=94.60, val_obj_accuracy=93.60, train_loss=8.380, train_class_accuracy=79.20, train_no_obj_accuracy=91.20, train_obj_accuracy=96.00]\n",
      "Epoch 24: 100%|██████████| 518/518 [06:01<00:00,  1.43it/s, v_num=314, val_loss=3.500, val_class_accuracy=86.70, val_no_obj_accuracy=93.60, val_obj_accuracy=95.20, train_loss=8.010, train_class_accuracy=80.50, train_no_obj_accuracy=91.50, train_obj_accuracy=96.10]\n",
      "Epoch 25: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=3.730, val_class_accuracy=85.50, val_no_obj_accuracy=94.80, val_obj_accuracy=94.10, train_loss=7.780, train_class_accuracy=81.30, train_no_obj_accuracy=91.70, train_obj_accuracy=96.20]\n",
      "Epoch 26: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=3.350, val_class_accuracy=87.80, val_no_obj_accuracy=94.00, val_obj_accuracy=95.80, train_loss=7.600, train_class_accuracy=81.80, train_no_obj_accuracy=91.80, train_obj_accuracy=96.20]\n",
      "Epoch 27: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=3.260, val_class_accuracy=88.10, val_no_obj_accuracy=94.30, val_obj_accuracy=95.40, train_loss=7.330, train_class_accuracy=82.90, train_no_obj_accuracy=92.00, train_obj_accuracy=96.30]\n",
      "Epoch 28: 100%|██████████| 518/518 [06:03<00:00,  1.43it/s, v_num=314, val_loss=3.320, val_class_accuracy=87.30, val_no_obj_accuracy=93.70, val_obj_accuracy=95.70, train_loss=7.040, train_class_accuracy=83.80, train_no_obj_accuracy=92.20, train_obj_accuracy=96.50]\n",
      "Epoch 29: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=3.210, val_class_accuracy=88.50, val_no_obj_accuracy=94.70, val_obj_accuracy=95.10, train_loss=6.860, train_class_accuracy=84.40, train_no_obj_accuracy=92.40, train_obj_accuracy=96.60]\n",
      "Epoch 30: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=3.140, val_class_accuracy=88.70, val_no_obj_accuracy=94.60, val_obj_accuracy=95.80, train_loss=6.650, train_class_accuracy=85.20, train_no_obj_accuracy=92.60, train_obj_accuracy=96.60]\n",
      "Epoch 31: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=3.020, val_class_accuracy=89.40, val_no_obj_accuracy=94.20, val_obj_accuracy=96.20, train_loss=6.330, train_class_accuracy=86.30, train_no_obj_accuracy=92.80, train_obj_accuracy=96.80]\n",
      "Epoch 32: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=2.980, val_class_accuracy=89.40, val_no_obj_accuracy=95.10, val_obj_accuracy=95.80, train_loss=6.160, train_class_accuracy=86.80, train_no_obj_accuracy=92.90, train_obj_accuracy=96.80]\n",
      "Epoch 33: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=2.910, val_class_accuracy=90.40, val_no_obj_accuracy=95.60, val_obj_accuracy=95.40, train_loss=5.940, train_class_accuracy=87.60, train_no_obj_accuracy=93.10, train_obj_accuracy=96.90]\n",
      "Epoch 34: 100%|██████████| 518/518 [06:03<00:00,  1.42it/s, v_num=314, val_loss=2.780, val_class_accuracy=90.70, val_no_obj_accuracy=94.80, val_obj_accuracy=96.30, train_loss=5.720, train_class_accuracy=88.30, train_no_obj_accuracy=93.30, train_obj_accuracy=97.10]\n",
      "Epoch 35: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=2.790, val_class_accuracy=90.70, val_no_obj_accuracy=95.20, val_obj_accuracy=95.90, train_loss=5.500, train_class_accuracy=89.10, train_no_obj_accuracy=93.50, train_obj_accuracy=97.10]\n",
      "Epoch 36: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=2.710, val_class_accuracy=91.30, val_no_obj_accuracy=95.40, val_obj_accuracy=95.80, train_loss=5.310, train_class_accuracy=89.70, train_no_obj_accuracy=93.60, train_obj_accuracy=97.20]\n",
      "Epoch 37: 100%|██████████| 518/518 [06:03<00:00,  1.43it/s, v_num=314, val_loss=2.700, val_class_accuracy=91.30, val_no_obj_accuracy=95.50, val_obj_accuracy=95.90, train_loss=5.130, train_class_accuracy=90.20, train_no_obj_accuracy=93.80, train_obj_accuracy=97.30]\n",
      "Epoch 38: 100%|██████████| 518/518 [06:02<00:00,  1.43it/s, v_num=314, val_loss=2.680, val_class_accuracy=91.60, val_no_obj_accuracy=95.40, val_obj_accuracy=96.10, train_loss=4.910, train_class_accuracy=91.00, train_no_obj_accuracy=93.90, train_obj_accuracy=97.40]\n",
      "Epoch 39: 100%|██████████| 518/518 [06:01<00:00,  1.43it/s, v_num=314, val_loss=2.600, val_class_accuracy=91.90, val_no_obj_accuracy=95.70, val_obj_accuracy=95.70, train_loss=4.780, train_class_accuracy=91.50, train_no_obj_accuracy=94.00, train_obj_accuracy=97.40]\n",
      "Epoch 39: 100%|██████████| 518/518 [06:20<00:00,  1.36it/s, v_num=314, val_loss=2.640, val_class_accuracy=91.80, val_no_obj_accuracy=95.90, val_obj_accuracy=95.50, train_loss=4.640, train_class_accuracy=91.50, train_no_obj_accuracy=94.00, train_obj_accuracy=97.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 518/518 [06:27<00:00,  1.34it/s, v_num=314, val_loss=2.640, val_class_accuracy=91.80, val_no_obj_accuracy=95.90, val_obj_accuracy=95.50, train_loss=4.640, train_class_accuracy=91.50, train_no_obj_accuracy=94.00, train_obj_accuracy=97.40]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "\n",
    "class MyProgressBar(TQDMProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = super().init_validation_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    def init_predict_tqdm(self):\n",
    "        bar = super().init_predict_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    # def init_test_tqdm(self):\n",
    "    #     bar = super().init_test_tqdm()\n",
    "    #     if not sys.stdout.isatty():\n",
    "    #         bar.disable = True\n",
    "    #     return bar\n",
    "\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"yolov3\")\n",
    "\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# from pytorch_lightning.strategies import Fabric\n",
    "\n",
    "# training\n",
    "trainer = pl.Trainer(\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[\n",
    "        MyProgressBar(refresh_rate=1),\n",
    "        # RichProgressBar(refresh_rate=1, leave=False),\n",
    "        LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"ckpt_logs/yolov3\",\n",
    "            save_top_k=3,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            filename=\"model-{epoch:02d}-{val_loss:.2f}-{val_loss:4f}\",\n",
    "            save_last=True,\n",
    "        ),\n",
    "    ],\n",
    "    logger=logger,\n",
    "    precision=16,  # \"16-mixed\",\n",
    "    accelerator=\"gpu\",\n",
    "    devices=\"auto\",\n",
    "    strategy=\"ddp_notebook\",\n",
    "    check_val_every_n_epoch=1,\n",
    "    # limit_train_batches=5,\n",
    "    # limit_val_batches=1,\n",
    "    # limit_test_batches=1,\n",
    "    max_epochs=config.NUM_EPOCHS * 2 // 5,\n",
    "    # max_epochs=10,\n",
    ")\n",
    "\n",
    "# # ******************************\n",
    "\n",
    "# from pytorch_lightning.tuner import Tuner\n",
    "\n",
    "# tuner = Tuner(trainer)\n",
    "\n",
    "# ## Find Batch Size *************\n",
    "\n",
    "# batch_size = tuner.scale_batch_size(model, datamodule=data_module, mode=\"binsearch\")\n",
    "\n",
    "# print(batch_size)\n",
    "\n",
    "# # Found GPU can handle 132 Batch Size will use 128\n",
    "\n",
    "# ## Find LR *********************\n",
    "\n",
    "# lr_finder = tuner.lr_find(model, datamodule=data_module, num_training=200)\n",
    "\n",
    "# # Results can be found in\n",
    "# print(lr_finder.results)\n",
    "\n",
    "# # Plot with\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "\n",
    "# # Pick point based on plot, or get a suggestion\n",
    "# new_lr = lr_finder.suggestion()\n",
    "\n",
    "# # update hparams of the model\n",
    "# model.hparams.lr = new_lr\n",
    "\n",
    "# # ******************************\n",
    "\n",
    "# Uncomment the following line to train the model\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42e0cd17-dd73-4d7f-99b4-2dadc83ad193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO:pytorch_lightning.utilities.rank_zero:----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 518/518 [31:37<00:00,  3.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         map_epoch         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7876859903335571     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_class_accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     96.89830017089844     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test_no_obj_accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     96.0931396484375      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_obj_accuracy     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     97.3210678100586      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        map_epoch        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7876859903335571    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_class_accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    96.89830017089844    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test_no_obj_accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    96.0931396484375     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_obj_accuracy    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    97.3210678100586     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'map_epoch': 0.7876859903335571,\n",
       "  'test_class_accuracy': 96.89830017089844,\n",
       "  'test_no_obj_accuracy': 96.0931396484375,\n",
       "  'test_obj_accuracy': 97.3210678100586}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(\n",
    "    model,\n",
    "    # dataloaders=val_dataloader,\n",
    "    datamodule=data_module,\n",
    "    # ckpt_path=\"ckpt_logs/yolov3/last.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7405ce9c-812a-4add-bfcc-1c00459f0c47",
   "metadata": {},
   "source": [
    "### Load Model from CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a840897a-db6e-415a-b3e0-535241293a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parrotletml.yoloplnet import LitYOLONet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb7c022-3e88-45e5-8094-f19f93da3f66",
   "metadata": {},
   "source": [
    "#### Load Model From CheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aaa1a26-be53-4903-89ef-a6b6d9a38e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Checkpoint\n"
     ]
    }
   ],
   "source": [
    "loaded_model = LitYOLONet.load_from_checkpoint(\"ckpt_logs/yolov3/last.ckpt\")\n",
    "\n",
    "# final-model-epoch=39-val_loss=3.16-val_loss=3.162749.ckpt\n",
    "# final-model-epoch=39-val_loss=3.18-val_loss=3.178922.ckpt ## Multi Image Size 10, 13\n",
    "# final-model-epoch=39-val_loss=2.27-val_loss=2.265536.ckpt\n",
    "\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Loaded Checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff597c14-e1dd-4838-a3ce-69a55ae618bf",
   "metadata": {},
   "source": [
    "i#### Save Model to Multiple Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850fd1ca-63e2-4cbb-a8bf-0b5e65fbcbd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39mto_onnx(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt_logs/yolov3/yolov3.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 3\u001b[0m     input_sample\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m416\u001b[39m, \u001b[38;5;241m416\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m      4\u001b[0m     export_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     input_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     output_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(loaded_model\u001b[38;5;241m.\u001b[39myolo\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt_logs/yolov3/yolov3.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loaded_model.to_onnx(\n",
    "    \"ckpt_logs/yolov3/yolov3.onnx\",\n",
    "    input_sample=torch.empty(1, 3, 416, 416, dtype=torch.float32),\n",
    "    export_params=True,\n",
    "    input_names=[\"image\"],\n",
    "    output_names=[\"classes\"],\n",
    ")\n",
    "\n",
    "torch.save(loaded_model.yolo.state_dict(), \"ckpt_logs/yolov3/yolov3.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d6913-e895-4854-bf90-27e3f097cf21",
   "metadata": {},
   "source": [
    "#### Load Model in ONNX and PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7ef8a-5b96-44fa-9a8a-aecec3f756aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class config:\n",
    "    DEVICE = \"cuda\"  # \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    mean = [0.4914, 0.4822, 0.4465]\n",
    "    std = [0.2470, 0.2435, 0.2616]\n",
    "\n",
    "    IMAGE_SIZE = 416\n",
    "    \n",
    "    ANCHORS = [\n",
    "        [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "        [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "        [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "    ]  # Note these have been rescaled to be between [0, 1]\n",
    "    S = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8]\n",
    "\n",
    "    NUM_CLASSES = 20\n",
    "\n",
    "    PASCAL_CLASSES = [\n",
    "        \"aeroplane\",\n",
    "        \"bicycle\",\n",
    "        \"bird\",\n",
    "        \"boat\",\n",
    "        \"bottle\",\n",
    "        \"bus\",\n",
    "        \"car\",\n",
    "        \"cat\",\n",
    "        \"chair\",\n",
    "        \"cow\",\n",
    "        \"diningtable\",\n",
    "        \"dog\",\n",
    "        \"horse\",\n",
    "        \"motorbike\",\n",
    "        \"person\",\n",
    "        \"pottedplant\",\n",
    "        \"sheep\",\n",
    "        \"sofa\",\n",
    "        \"train\",\n",
    "        \"tvmonitor\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911b12f7-364f-4881-ab20-80369ffbcd5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m onnxmodel \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mInferenceSession(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt_logs/yolov3/yolov3.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the PT Model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m ptmodel \u001b[38;5;241m=\u001b[39m YOLOv3(num_classes\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mNUM_CLASSES)\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m      9\u001b[0m ptmodel\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m     10\u001b[0m     torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt_logs/yolov3/yolov3.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m ptmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "from parrotletml.model import YOLOv3\n",
    "\n",
    "# Load the ONNX model\n",
    "onnxmodel = ort.InferenceSession(\"ckpt_logs/yolov3/yolov3.onnx\")\n",
    "\n",
    "# Load the PT Model\n",
    "ptmodel = YOLOv3(num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
    "ptmodel.load_state_dict(\n",
    "    torch.load(\"ckpt_logs/yolov3/yolov3.pth\", map_location=config.DEVICE)\n",
    ")\n",
    "ptmodel.eval()\n",
    "\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70880bf-5964-4b71-b34d-ac23cdc59265",
   "metadata": {},
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afede8b3-2a76-4b31-ade9-99bcc304727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from parrotletml.mygradcam import YoloCAM\n",
    "from parrotletml.utils import cells_to_bboxes, overlay_predictions, non_max_suppression\n",
    "\n",
    "import glob\n",
    "\n",
    "from parrotletml.utils import resize_image, normalize_image\n",
    "\n",
    "\n",
    "scaled_anchors = torch.tensor(config.ANCHORS, device=config.DEVICE) * torch.tensor(\n",
    "    config.S, device=config.DEVICE\n",
    ").unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "\n",
    "cam = YoloCAM(model=ptmodel, target_layers=[ptmodel.layers[-3]], use_cuda=False)\n",
    "\n",
    "\n",
    "# Define the preprocessing function for the input image\n",
    "def preprocess_image(image):\n",
    "    # # Read the image using OpenCV and convert to RGB\n",
    "    # image = cv2.cvtColor(cv2.imread(impath, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image = np.asarray(image)\n",
    "\n",
    "    # Resize, normalize, and convert image to the required format\n",
    "    image = resize_image(image, (416, 416), fill=(0, 0, 0))\n",
    "\n",
    "    # image = np.expand_dims(normalize_cifar_cv_image(resize_image(impath, (32, 32)).transpose(2, 0, 1)).astype(np.float32), axis=0)\n",
    "    # image = np.expand_dims(\n",
    "    #     normalize_image(image, mean=[0, 0, 0], std=[1, 1, 1])\n",
    "    #     .transpose(2, 0, 1)\n",
    "    #     .astype(np.float32),\n",
    "    #     axis=0,\n",
    "    # )\n",
    "\n",
    "    image = normalize_image(image, mean=[0, 0, 0], std=[1, 1, 1]).astype(np.float32)\n",
    "\n",
    "    return image, torch.from_numpy(image.transpose(2, 0, 1))\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict(\n",
    "    image: np.ndarray,\n",
    "    iou_thresh: float = 0.5,\n",
    "    thresh: float = 0.4,\n",
    "    show_cam: bool = True,\n",
    "    transparency: float = 0.3,\n",
    ") -> List[np.ndarray]:\n",
    "    # transformed_image = torch.tensor(\n",
    "    #     config.transforms(image=image)[\"image\"].unsqueeze(0), device=config.DEVICE\n",
    "    # )\n",
    "\n",
    "    # print(transformed_image.shape)\n",
    "\n",
    "    image, transformed_image = preprocess_image(image)\n",
    "    transformed_image = torch.tensor(transformed_image, device=config.DEVICE).unsqueeze(\n",
    "        0\n",
    "    )\n",
    "    # print(transformed_image.shape)\n",
    "\n",
    "    output = ptmodel(transformed_image)\n",
    "\n",
    "    bboxes = [[] for _ in range(1)]\n",
    "    for i in range(3):\n",
    "        batch_size, A, S, _, _ = output[i].shape\n",
    "        anchor = scaled_anchors[i]\n",
    "        boxes_scale_i = cells_to_bboxes(\n",
    "            torch.tensor(output[i], device=config.DEVICE), anchor, S=S, is_preds=True\n",
    "        )\n",
    "        for idx, (box) in enumerate(boxes_scale_i):\n",
    "            bboxes[idx] += box\n",
    "\n",
    "    nms_boxes = non_max_suppression(\n",
    "        bboxes[0],\n",
    "        iou_threshold=iou_thresh,\n",
    "        threshold=thresh,\n",
    "        box_format=\"midpoint\",\n",
    "    )\n",
    "    plot_img = np.clip(\n",
    "        overlay_predictions(\n",
    "            image,\n",
    "            nms_boxes,\n",
    "            class_labels=config.PASCAL_CLASSES,\n",
    "            confidence_threshold=0.75,\n",
    "        ),\n",
    "        0,\n",
    "        1,\n",
    "    )\n",
    "    if not show_cam:\n",
    "        return [plot_img, image]\n",
    "\n",
    "    grayscale_cam = cam(transformed_image, scaled_anchors)[0, :, :]\n",
    "    # img = cv2.resize(image, (416, 416))\n",
    "    # img = np.float32(img) / 255\n",
    "    cam_image = show_cam_on_image(\n",
    "        image,\n",
    "        grayscale_cam,\n",
    "        use_rgb=True,\n",
    "        image_weight=transparency,\n",
    "    )\n",
    "    return [plot_img, image, cam_image]\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for p in glob.glob(\"samples/*.*g\"):\n",
    "        img_path = p  # \"samples/ammar.jpg\"\n",
    "        image = np.array(Image.open(img_path))\n",
    "\n",
    "        # image = resize_image(image, (416, 416), fill=(0, 0, 0))\n",
    "\n",
    "        pimage = predict(image)\n",
    "\n",
    "        # Create a figure with two subplots\n",
    "        fig, (ax2, ax3) = plt.subplots(\n",
    "            1, 2, figsize=(7, 10)\n",
    "        )  # Adjust fig size as needed\n",
    "\n",
    "        # Plot the images in the subplots\n",
    "        # ax1.imshow(image)\n",
    "        # ax1.set_title(\"Original Image\")\n",
    "        # ax1.axis(\"off\")  # Turn off-axis labels and ticks\n",
    "\n",
    "        # Plot the images in the subplots\n",
    "        ax2.imshow(pimage[0])\n",
    "        ax2.set_title(\"Overlayed Image\")\n",
    "        ax2.axis(\"off\")  # Turn off-axis labels and ticks\n",
    "\n",
    "        ax3.imshow(pimage[2])\n",
    "        ax3.set_title(\"GradCam\")\n",
    "        ax3.axis(\"off\")  # Turn off-axis labels and ticks\n",
    "\n",
    "        # ax4.imshow(pimage[1])\n",
    "        # ax4.set_title(\"Transformed\")\n",
    "        # ax4.axis(\"off\")  # Turn off-axis labels and ticks\n",
    "\n",
    "        # Display the figure\n",
    "        plt.tight_layout()  # Adjust layout for better spacing\n",
    "        plt.show()\n",
    "\n",
    "    # cv2.imshow(\"image\", image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba07f1-fb71-4eca-b631-eb5fc7ab8914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc365d10-dbec-4abb-8ed3-1d806f8e37a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84214d39-0dc9-43ba-962d-0945274a257c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dafd4a-26b3-4b69-8842-d8185ba6aebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eraenv",
   "language": "python",
   "name": "eraenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
